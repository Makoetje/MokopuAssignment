import streamlit as st
import joblib
import pandas as pd
import numpy as np
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import io
import subprocess
import sys
import re

# -------------------------------------------------
# Page config & styling
# -------------------------------------------------
st.set_page_config(
    page_title="Student Dropout Predictor", 
    layout="wide"
)

st.markdown("""
<style>
    .main-header {
        font-size: 3rem !important;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .risk-high {
        background-color: #ffcccc;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #ff0000;
    }
    .risk-medium {
        background-color: #fff4cc;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #ffa500;
    }
    .risk-low {
        background-color: #ccffcc;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #008000;
    }
    .feature-card {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0px;
    }
    .training-output {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 10px;
        border-left: 5px solid #6c757d;
        font-family: monospace;
        font-size: 0.9em;
        white-space: pre-wrap;
        max-height: 400px;
        overflow-y: auto;
    }
</style>
""", unsafe_allow_html=True)

# -------------------------------------------------
# Enhanced functions to read training outputs
# -------------------------------------------------
@st.cache_resource(show_spinner=False)
def load_artifacts():
    """Load model components with better error handling"""
    try:
        required_files = [
            "model/rf_model.pkl", 
            "model/scaler.pkl", 
            "model/encoder.pkl", 
            "model/features.txt"
        ]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        if missing_files:
            st.error(f"Missing required files: {', '.join(missing_files)}")
            st.info("Please run the training script first: `python dropout_random_forest_clean.py`")
            return None, None, None, None, None, None
        
        with st.spinner("Loading AI model components..."):
            model = joblib.load("model/rf_model.pkl")
            scaler = joblib.load("model/scaler.pkl")
            encoder = joblib.load("model/encoder.pkl")
            
            # Load features with better error handling
            try:
                with open("model/features.txt", "r", encoding='utf-8') as f:
                    features = [line.strip() for line in f.readlines() if line.strip()]
            except:
                # If features.txt is empty or corrupted, try to get features from model
                features = []
                if hasattr(model, 'feature_importances_'):
                    # Try to load from selected_features.csv
                    if os.path.exists("model/selected_features.csv"):
                        selected_df = pd.read_csv("model/selected_features.csv")
                        features = selected_df['Selected_Features'].tolist()
            
            # Load evaluation results if available
            if os.path.exists("model/evaluation_results.pkl"):
                eval_res = joblib.load("model/evaluation_results.pkl")
            else:
                eval_res = {}
            
            # Load training outputs from files
            training_outputs = load_training_outputs()
            
        st.success(f"All model artifacts loaded successfully! Found {len(features)} features.")
        return model, scaler, encoder, features, eval_res, training_outputs
        
    except Exception as e:
        st.error(f"Error loading model: {str(e)}")
        import traceback
        st.error(f"Detailed error: {traceback.format_exc()}")
        return None, None, None, None, None, None

def load_training_outputs():
    """Load training outputs from various files generated by the training script"""
    outputs = {
        'missing_values': None,
        'outlier_stats': None,
        'feature_selection': None,
        'model_performance': None,
        'confusion_matrix': None,
        'training_log': ""
    }
    
    # Load missing values report
    if os.path.exists('missing_values_report.csv'):
        try:
            outputs['missing_values'] = pd.read_csv('missing_values_report.csv')
        except:
            outputs['missing_values'] = None
    
    # Load clean dataset to get stats
    dataset_stats = {}
    if os.path.exists('clean_student_data.csv'):
        try:
            clean_df = pd.read_csv('clean_student_data.csv')
            dataset_stats['final_shape'] = clean_df.shape
            if len(clean_df.columns) > 0:
                target_col = clean_df.columns[0]
                dataset_stats['target_distribution'] = clean_df[target_col].value_counts().to_dict()
        except:
            pass
    
    # Also try to load from original data
    if not dataset_stats.get('target_distribution') and os.path.exists('data.csv'):
        try:
            original_df = pd.read_csv('data.csv')
            target_candidates = [c for c in original_df.columns if c.lower() in ('target', 'status', 'outcome')]
            if target_candidates:
                target_col = target_candidates[0]
                dataset_stats['target_distribution'] = original_df[target_col].value_counts().to_dict()
                dataset_stats['original_shape'] = original_df.shape
        except:
            pass
    
    outputs['dataset_stats'] = dataset_stats
    
    # Generate training log from available data
    training_log = generate_training_log()
    outputs['training_log'] = training_log
    
    # Parse key metrics from training log
    outputs['parsed_metrics'] = parse_training_metrics(training_log)
    
    return outputs

def generate_training_log():
    """Generate training log from available files and data"""
    log_parts = []
    
    # Dataset info
    if os.path.exists('data.csv'):
        try:
            df_original = pd.read_csv('data.csv')
            log_parts.append(f"TASK 1: Loading dataset")
            log_parts.append(f"  Dataset shape: {df_original.shape}")
            
            # Add target distribution if available
            target_candidates = [c for c in df_original.columns if c.lower() in ('target', 'status', 'outcome')]
            if target_candidates:
                target_col = target_candidates[0]
                target_dist = df_original[target_col].value_counts()
                log_parts.append(f"  Target distribution:")
                for cls, count in target_dist.items():
                    log_parts.append(f"    {cls}: {count}")
        except:
            pass
    
    # Model performance from evaluation results
    if os.path.exists('model/evaluation_results.pkl'):
        try:
            eval_res = joblib.load('model/evaluation_results.pkl')
            log_parts.append(f"\nTASK 7: Model Training & Evaluation")
            if 'best_params' in eval_res:
                log_parts.append(f"  Best Params: {eval_res['best_params']}")
            if 'holdout_accuracy' in eval_res:
                log_parts.append(f"  Holdout Accuracy: {eval_res['holdout_accuracy']:.3f}")
        except:
            pass
    
    return "\n".join(log_parts)

def parse_training_metrics(log_text):
    """Parse key metrics from training log text"""
    metrics = {}
    
    # Parse dataset shape
    shape_match = re.search(r'Dataset shape: \((\d+), (\d+)\)', log_text)
    if shape_match:
        metrics['original_shape'] = (int(shape_match.group(1)), int(shape_match.group(2)))
    
    # Parse accuracy scores
    accuracy_match = re.search(r'Holdout Accuracy: (\d+\.\d+)', log_text)
    if accuracy_match:
        metrics['holdout_accuracy'] = float(accuracy_match.group(1))
    
    return metrics

def run_training_script():
    """Run the training script and capture output with proper encoding"""
    try:
        with st.spinner("Running training script... This may take a few minutes."):
            # Set environment for UTF-8 encoding
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            
            result = subprocess.run([
                sys.executable, "dropout_random_forest_clean.py"
            ], capture_output=True, text=True, timeout=300, env=env, encoding='utf-8')
            
            if result.returncode == 0:
                st.success("Training completed successfully!")
                # Clear cache to reload artifacts
                st.cache_resource.clear()
                return result.stdout
            else:
                st.error(f"Training failed: {result.stderr}")
                return None
    except subprocess.TimeoutExpired:
        st.error("Training timed out after 5 minutes")
        return None
    except Exception as e:
        st.error(f"Error running training: {str(e)}")
        return None

# Load model components
model, scaler, encoder, features, eval_res, training_outputs = load_artifacts()

# -------------------------------------------------
# Sidebar Navigation
# -------------------------------------------------
st.sidebar.markdown("## Student Dropout Predictor")
st.sidebar.markdown("---")

# Show warning if model failed to load
if model is None:
    st.sidebar.error("Model not loaded")
    st.sidebar.info("Please run the training script first from the 'Retrain Model' page")
else:
    st.sidebar.success(f"Model loaded: {len(features)} features")

page = st.sidebar.radio(
    "Navigation",
    ["Single Prediction", "Batch Prediction", "Model Analysis", "Feature Insights", "Training Outputs", "Retrain Model"]
)

# Model info in sidebar with dynamic data
if model is not None:
    st.sidebar.markdown("---")
    st.sidebar.markdown("### Model Performance")

    # Use parsed metrics or fallback to eval_res
    parsed_metrics = training_outputs.get('parsed_metrics', {})
    cv_score = parsed_metrics.get('cv_mean', eval_res.get('cv_f1_macro_mean'))
    holdout_acc = parsed_metrics.get('holdout_accuracy', eval_res.get('holdout_accuracy'))

    if cv_score is not None:
        st.sidebar.metric("CV Score", f"{cv_score:.1%}")
    else:
        st.sidebar.metric("CV Score", "N/A")

    if holdout_acc is not None:
        st.sidebar.metric("Holdout Accuracy", f"{holdout_acc:.1%}")
    else:
        st.sidebar.metric("Holdout Accuracy", "N/A")

    st.sidebar.markdown("### Model Details")
    st.sidebar.write(f"**Features:** {len(features)}")
    if hasattr(encoder, 'classes_'):
        st.sidebar.write(f"**Classes:** {list(encoder.classes_)}")
    else:
        st.sidebar.write("**Classes:** N/A")
    st.sidebar.write(f"**Last Updated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}")

# -------------------------------------------------
# PAGE 1: Single Prediction
# -------------------------------------------------
if page == "Single Prediction":
    st.markdown('<h1 class="main-header">Student Dropout Risk Predictor</h1>', unsafe_allow_html=True)
    
    if model is None:
        st.error("Model not loaded. Please train the model first from the 'Retrain Model' page.")
        st.info("The model needs to be trained before making predictions.")
    elif not features:
        st.error("No features found. The model appears to be corrupted.")
        st.info("Please retrain the model from the 'Retrain Model' page.")
    else:
        st.markdown("### Enter Student Information for Prediction")
        
        # Create sample feature values for demonstration
        sample_features = {}
        for feature in features:
            if 'grade' in feature.lower():
                sample_features[feature] = 12.0
            elif any(x in feature for x in ['approved', 'enrolled', 'evaluations']):
                sample_features[feature] = 6
            elif any(x in feature for x in ['tuition', 'debtor', 'scholarship', 'gender']):
                sample_features[feature] = 1
            elif 'age' in feature.lower():
                sample_features[feature] = 20
            elif any(x in feature for x in ['gdp', 'rate']):
                sample_features[feature] = 1.5
            else:
                sample_features[feature] = 0
        
        with st.form("student_prediction_form"):
            student_data = {}
            
            # Organize features by category
            academic_features = [f for f in features if any(x in f.lower() for x in ['sem', 'grade', 'approved', 'enrolled'])]
            financial_features = [f for f in features if any(x in f.lower() for x in ['tuition', 'debtor', 'scholarship'])]
            personal_features = [f for f in features if any(x in f.lower() for x in ['age', 'gender', 'course', 'application'])]
            family_features = [f for f in features if any(x in f.lower() for x in ['mother', 'father', 'qualification', 'occupation'])]
            economic_features = [f for f in features if any(x in f.lower() for x in ['gdp', 'unemployment', 'inflation'])]
            
            # Academic Performance
            if academic_features:
                st.markdown("#### Academic Performance")
                col1, col2 = st.columns(2)
                
                # First Semester
                with col1:
                    st.markdown("**First Semester**")
                    first_sem_features = [f for f in academic_features if '1st' in f.lower()]
                    for feature in first_sem_features:
                        if 'grade' in feature.lower():
                            student_data[feature] = st.slider(
                                feature, 
                                0.0, 20.0, 
                                float(sample_features.get(feature, 12.0)), 
                                0.1
                            )
                        elif any(x in feature for x in ['approved', 'enrolled', 'evaluations']):
                            student_data[feature] = st.number_input(
                                feature, 
                                0, 20, 
                                int(sample_features.get(feature, 6))
                            )
                
                # Second Semester
                with col2:
                    st.markdown("**Second Semester**")
                    second_sem_features = [f for f in academic_features if '2nd' in f.lower()]
                    for feature in second_sem_features:
                        if 'grade' in feature.lower():
                            student_data[feature] = st.slider(
                                feature, 
                                0.0, 20.0, 
                                float(sample_features.get(feature, 12.0)), 
                                0.1
                            )
                        elif any(x in feature for x in ['approved', 'enrolled', 'evaluations']):
                            student_data[feature] = st.number_input(
                                feature, 
                                0, 20, 
                                int(sample_features.get(feature, 6))
                            )
            
            # Financial Info
            if financial_features:
                st.markdown("---")
                st.markdown("#### Financial Status")
                fin_cols = st.columns(min(3, len(financial_features)))
                for idx, feature in enumerate(financial_features):
                    with fin_cols[idx % len(fin_cols)]:
                        if 'tuition' in feature.lower():
                            default_val = int(sample_features.get(feature, 1))
                            student_data[feature] = st.selectbox(
                                feature, 
                                options=[1, 0], 
                                format_func=lambda x: "Up to date" if x == 1 else "Not up to date", 
                                index=0 if default_val == 1 else 1
                            )
                        elif 'debtor' in feature.lower():
                            default_val = int(sample_features.get(feature, 0))
                            student_data[feature] = st.selectbox(
                                feature, 
                                options=[0, 1], 
                                format_func=lambda x: "Yes" if x == 1 else "No", 
                                index=0 if default_val == 0 else 1
                            )
                        elif 'scholarship' in feature.lower():
                            default_val = int(sample_features.get(feature, 1))
                            student_data[feature] = st.selectbox(
                                feature, 
                                options=[1, 0], 
                                format_func=lambda x: "Yes" if x == 1 else "No", 
                                index=0 if default_val == 1 else 1
                            )
                        else:
                            student_data[feature] = st.number_input(
                                feature, 
                                0, 1, 
                                int(sample_features.get(feature, 0))
                            )
            
            # Personal Info
            if personal_features:
                st.markdown("---")
                st.markdown("#### Personal Information")
                personal_cols = st.columns(min(3, len(personal_features)))
                for idx, feature in enumerate(personal_features):
                    with personal_cols[idx % len(personal_cols)]:
                        if 'age' in feature.lower():
                            student_data[feature] = st.number_input(
                                feature, 
                                15, 60, 
                                int(sample_features.get(feature, 20))
                            )
                        elif 'course' in feature.lower():
                            # Safe course selection with valid index
                            course_options = list(range(1, 18))
                            default_course = int(sample_features.get(feature, 1))
                            # Ensure default is within valid range
                            default_index = min(max(0, default_course - 1), len(course_options) - 1)
                            student_data[feature] = st.selectbox(
                                feature, 
                                options=course_options, 
                                format_func=lambda x: f"Course {x}", 
                                index=default_index
                            )
                        elif 'gender' in feature.lower():
                            default_val = int(sample_features.get(feature, 1))
                            student_data[feature] = st.selectbox(
                                feature, 
                                options=[0, 1], 
                                format_func=lambda x: "Female" if x == 0 else "Male", 
                                index=0 if default_val == 0 else 1
                            )
                        else:
                            student_data[feature] = st.number_input(
                                feature, 
                                0, 100, 
                                int(sample_features.get(feature, 0))
                            )
            
            # Family Background
            if family_features:
                st.markdown("---")
                st.markdown("#### Family Background")
                family_cols = st.columns(min(3, len(family_features)))
                for idx, feature in enumerate(family_features):
                    with family_cols[idx % len(family_cols)]:
                        student_data[feature] = st.number_input(
                            feature, 
                            0, 20, 
                            int(sample_features.get(feature, 0))
                        )
            
            # Economic Factors
            if economic_features:
                st.markdown("---")
                st.markdown("#### Economic Factors")
                economic_cols = st.columns(min(3, len(economic_features)))
                for idx, feature in enumerate(economic_features):
                    with economic_cols[idx % len(economic_cols)]:
                        if any(x in feature.lower() for x in ['gdp', 'rate']):
                            student_data[feature] = st.number_input(
                                feature, 
                                -10.0, 20.0, 
                                float(sample_features.get(feature, 1.5)), 
                                0.1
                            )
                        else:
                            student_data[feature] = st.number_input(
                                feature, 
                                0.0, 50.0, 
                                float(sample_features.get(feature, 0.0)), 
                                0.1
                            )
            
            # Fill any remaining features with default values
            for feature in features:
                if feature not in student_data:
                    student_data[feature] = sample_features.get(feature, 0)
            
            submitted = st.form_submit_button("Predict Dropout Risk", use_container_width=True)
        
        # Prediction logic (outside the form)
        if submitted:
            st.markdown("---")
            st.header("Prediction Results")
            with st.spinner("Analyzing student data..."):
                try:
                    df_input = pd.DataFrame([student_data])
                    # Reindex to ensure all features are present
                    df_input = df_input.reindex(columns=features, fill_value=0)
                    # Scale features
                    X_scaled = scaler.transform(df_input)
                    # Predict
                    prediction_enc = model.predict(X_scaled)[0]
                    probabilities = model.predict_proba(X_scaled)[0]
                    prediction_label = encoder.inverse_transform([prediction_enc])[0]
                    
                    # Display result
                    col1, col2 = st.columns([1, 2])
                    with col1:
                        st.subheader("Risk Assessment")
                        if prediction_label == "Dropout":
                            st.markdown('<div class="risk-high">', unsafe_allow_html=True)
                            st.error("## HIGH RISK: DROPOUT")
                            st.warning("Immediate intervention recommended")
                            st.markdown("""**Recommended Actions:**\n- Schedule immediate academic counseling\n- Assign dedicated mentor support\n- Review financial aid status\n- Implement academic recovery plan""")
                            st.markdown('</div>', unsafe_allow_html=True)
                        elif prediction_label == "Enrolled":
                            st.markdown('<div class="risk-medium">', unsafe_allow_html=True)
                            st.warning("## MODERATE RISK: ENROLLED") 
                            st.info("Monitor closely and provide support")
                            st.markdown("""**Recommended Actions:**\n- Regular progress check-ins\n- Academic support services\n- Early warning system monitoring""")
                            st.markdown('</div>', unsafe_allow_html=True)
                        else:
                            st.markdown('<div class="risk-low">', unsafe_allow_html=True)
                            st.success("## LOW RISK: GRADUATE")
                            st.info("Continue current support strategies")
                            st.markdown("""**Recommended Actions:**\n- Maintain academic excellence\n- Career planning & development\n- Leadership opportunities""")
                            st.markdown('</div>', unsafe_allow_html=True)
                        confidence = max(probabilities) * 100
                        st.metric("Model Confidence", f"{confidence:.1f}%")
                    
                    with col2:
                        st.subheader("Probability Distribution")
                        fig, ax = plt.subplots(figsize=(10, 6))
                        colors = ['#ff6b6b', '#ffd93d', '#6bcf7f']
                        bars = ax.bar(encoder.classes_, probabilities, color=colors, alpha=0.8)
                        ax.set_ylabel('Probability', fontweight='bold')
                        ax.set_ylim(0, 1)
                        ax.grid(axis='y', alpha=0.3)
                        for bar, prob in zip(bars, probabilities):
                            height = bar.get_height()
                            ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01, 
                                   f'{prob:.1%}', ha='center', va='bottom', fontweight='bold')
                        plt.xticks(rotation=0)
                        plt.tight_layout()
                        st.pyplot(fig)
                    
                    # Key Influencing Factors
                    st.markdown("---")
                    st.subheader("Key Influencing Factors")
                    if hasattr(model, 'feature_importances_'):
                        feature_importances = model.feature_importances_
                        contributions = {}
                        for i, feature in enumerate(features):
                            if feature in student_data:
                                contributions[feature] = student_data[feature] * feature_importances[i]
                        top_contributors = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)[:6]
                        cols = st.columns(3)
                        for idx, (feature, contribution) in enumerate(top_contributors):
                            with cols[idx % 3]:
                                value = student_data[feature]
                                if 'approved' in feature.lower():
                                    status = "Low" if value < 3 else "Good" if value >= 5 else "Average"
                                    color = "inverse" if value < 3 else "normal" if value >= 5 else "off"
                                elif 'grade' in feature.lower():
                                    status = "Low" if value < 10 else "Good" if value >= 14 else "Average"
                                    color = "inverse" if value < 10 else "normal" if value >= 14 else "off"
                                elif feature.lower() == 'tuition fees up to date':
                                    status = "Risk" if value == 0 else "Good"
                                    color = "inverse" if value == 0 else "normal"
                                elif feature.lower() == 'debtor':
                                    status = "Risk" if value == 1 else "Good"
                                    color = "inverse" if value == 1 else "normal"
                                elif 'age' in feature.lower():
                                    status = "Higher risk" if value > 25 else "Typical"
                                    color = "off" if value > 25 else "normal"
                                else:
                                    status = "Neutral"
                                    color = "off"
                                st.metric(label=feature, value=value, delta=status, delta_color=color)
                    
                except Exception as e:
                    st.error(f"Prediction error: {str(e)}")
                    import traceback
                    st.error(f"Detailed error: {traceback.format_exc()}")

# -------------------------------------------------
# PAGE 2: Batch Prediction
# -------------------------------------------------
elif page == "Batch Prediction":
    st.markdown('<h1 class="main-header">Batch Prediction</h1>', unsafe_allow_html=True)
    
    if model is None or not features:
        st.error("Model not loaded or no features found. Please train the model first from the 'Retrain Model' page.")
    else:
        st.markdown("Upload a CSV file with student data to get predictions for multiple students.")
        
        # Template download
        st.markdown("### Download Template")
        st.write("Use this template to format your data correctly:")
        
        # Create template with actual features
        template_data = {}
        for feature in features:
            if 'grade' in feature.lower():
                template_data[feature] = [12.0]
            elif any(x in feature for x in ['approved', 'enrolled', 'evaluations']):
                template_data[feature] = [6]
            elif any(x in feature for x in ['tuition', 'debtor', 'scholarship', 'gender']):
                template_data[feature] = [1]
            elif 'age' in feature.lower():
                template_data[feature] = [20]
            elif any(x in feature for x in ['gdp', 'rate']):
                template_data[feature] = [1.5]
            else:
                template_data[feature] = [0]
        
        template_df = pd.DataFrame(template_data)
        csv_template = template_df.to_csv(index=False)
        
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "Download CSV Template",
                data=csv_template,
                file_name="student_prediction_template.csv",
                mime="text/csv",
                use_container_width=True
            )
        with col2:
            with st.expander("Template Preview"):
                st.dataframe(template_df, use_container_width=True)
        
        st.markdown("---")
        uploaded_file = st.file_uploader("Upload your CSV file", type=["csv"])
        
        if uploaded_file is not None:
            try:
                df_upload = pd.read_csv(uploaded_file)
                st.success(f"Successfully uploaded {len(df_upload)} student records")
                
                # Check for required features
                missing_features = [f for f in features if f not in df_upload.columns]
                if missing_features:
                    st.error(f"Missing features: {', '.join(missing_features[:10])}{'...' if len(missing_features) > 10 else ''}")
                    st.info("Please use the template above and include all features.")
                else:
                    # Prepare data
                    X_upload = df_upload[features]
                    X_scaled = scaler.transform(X_upload)
                    
                    # Make predictions
                    predictions_enc = model.predict(X_scaled)
                    predictions = encoder.inverse_transform(predictions_enc)
                    probabilities = model.predict_proba(X_scaled)
                    confidence_scores = [max(prob) for prob in probabilities]

                    # Prepare results dataframe
                    results_df = df_upload.copy()
                    results_df['Prediction'] = predictions
                    results_df['Confidence'] = [f"{score:.1%}" for score in confidence_scores]
                    for i, class_name in enumerate(encoder.classes_):
                        results_df[f'Prob_{class_name}'] = [f"{prob[i]:.1%}" for prob in probabilities]

                    # Display prediction results
                    st.markdown("### Prediction Results")
                    st.dataframe(results_df, use_container_width=True)

                    # Download results
                    st.markdown("### Download Results")
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        results_df.to_excel(writer, index=False, sheet_name="Predictions")
                    
                    st.download_button(
                        "Download Excel Results",
                        data=output.getvalue(),
                        file_name="batch_predictions.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )

                    # Display Prediction Class Distribution
                    st.markdown("### Prediction Distribution")
                    pred_counts = results_df['Prediction'].value_counts()
                    fig1, ax1 = plt.subplots()
                    colors = ['#6bcf7f', '#ffd93d', '#ff6b6b']
                    ax1.pie(
                        pred_counts.values,
                        labels=pred_counts.index,
                        autopct='%1.1f%%',
                        colors=colors[:len(pred_counts)]
                    )
                    ax1.set_title("Prediction Class Distribution")
                    st.pyplot(fig1)

            except Exception as e:
                st.error(f"Error processing file: {str(e)}")

# -------------------------------------------------
# PAGE 3: Model Analysis
# -------------------------------------------------
elif page == "Model Analysis":
    st.markdown('<h1 class="main-header">Model Analysis</h1>', unsafe_allow_html=True)
    
    if model is None:
        st.error("Model not loaded. Please train the model first from the 'Retrain Model' page.")
    else:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Model Performance")
            
            # Performance metrics
            if eval_res:
                if 'holdout_accuracy' in eval_res:
                    st.metric("Holdout Accuracy", f"{eval_res['holdout_accuracy']:.3f}")
                if 'holdout_f1_weighted' in eval_res:
                    st.metric("F1 Score (Weighted)", f"{eval_res['holdout_f1_weighted']:.3f}")
                if 'cv_f1_macro_mean' in eval_res:
                    st.metric("CV F1-macro Mean", f"{eval_res['cv_f1_macro_mean']:.3f}")
                if 'cv_f1_macro_std' in eval_res:
                    st.metric("CV F1-macro Std", f"{eval_res['cv_f1_macro_std']:.3f}")
            else:
                st.info("No evaluation results available")
        
        with col2:
            st.subheader("Model Information")
            st.write(f"**Algorithm:** Random Forest")
            st.write(f"**Number of Trees:** {model.n_estimators}")
            st.write(f"**Number of Features:** {len(features)}")
            if hasattr(encoder, 'classes_'):
                st.write(f"**Classes:** {list(encoder.classes_)}")
            else:
                st.write("**Classes:** N/A")
            
            st.subheader("Top 10 Features")
            if hasattr(model, 'feature_importances_'):
                feature_importances = model.feature_importances_
                feat_imp_df = pd.DataFrame({"Feature": features, "Importance": feature_importances}).sort_values("Importance", ascending=False)
                for i, row in feat_imp_df.head(10).iterrows():
                    st.write(f"{i+1}. **{row['Feature']}** - {row['Importance']:.3f}")
                    st.progress(float(row['Importance']/feat_imp_df['Importance'].max()))
            else:
                st.info("Feature importances not available")
        
        st.markdown("---")
        st.subheader("Visualizations")
        
        viz_col1, viz_col2 = st.columns(2)
        
        with viz_col1:
            st.markdown("#### Confusion Matrix")
            if 'confusion_matrix' in eval_res:
                cm = np.array(eval_res['confusion_matrix'])
                fig, ax = plt.subplots(figsize=(8, 6))
                
                if hasattr(encoder, 'classes_'):
                    classes = encoder.classes_
                else:
                    classes = ['Dropout', 'Enrolled', 'Graduate']
                    
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                           xticklabels=classes, yticklabels=classes,
                           ax=ax)
                ax.set_xlabel('Predicted')
                ax.set_ylabel('Actual')
                ax.set_title('Confusion Matrix - Test Set')
                plt.tight_layout()
                st.pyplot(fig)
            else:
                st.info("Confusion matrix not available in evaluation results")
        
        with viz_col2:
            st.markdown("#### Feature Importance")
            if hasattr(model, 'feature_importances_'):
                feature_importances = model.feature_importances_
                feat_imp_df = pd.DataFrame({
                    'Feature': features, 
                    'Importance': feature_importances
                }).sort_values('Importance', ascending=True).tail(15)
                
                fig, ax = plt.subplots(figsize=(10, 8))
                bars = ax.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='skyblue')
                ax.set_xlabel('Importance Score')
                ax.set_title('Top 15 Most Important Features')
                
                for bar in bars:
                    width = bar.get_width()
                    ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, 
                           f'{width:.3f}', ha='left', va='center')
                
                plt.tight_layout()
                st.pyplot(fig)
            else:
                st.info("Feature importance visualization not available")
        
        # Additional Visualizations
        st.markdown("---")
        st.subheader("Additional Model Insights")
        
        col3, col4 = st.columns(2)
        
        with col3:
            st.markdown("#### Class Distribution")
            
            # Try multiple ways to get class distribution data
            class_distribution_data = None
            dataset_stats = training_outputs.get('dataset_stats', {})
            
            # Method 1: From dataset stats
            if dataset_stats.get('target_distribution'):
                class_distribution_data = dataset_stats['target_distribution']
            
            # Method 2: From evaluation results classification report
            if not class_distribution_data and 'classification_report' in eval_res:
                class_report = eval_res['classification_report']
                if hasattr(encoder, 'classes_'):
                    class_names = encoder.classes_
                    supports = []
                    valid_classes = []
                    for cls in class_names:
                        if str(cls) in class_report and 'support' in class_report[str(cls)]:
                            supports.append(class_report[str(cls)]['support'])
                            valid_classes.append(cls)
                    if supports:
                        class_distribution_data = dict(zip(valid_classes, supports))
            
            # Method 3: From clean dataset file
            if not class_distribution_data and os.path.exists('clean_student_data.csv'):
                try:
                    clean_df = pd.read_csv('clean_student_data.csv')
                    if len(clean_df.columns) > 0:
                        target_col = clean_df.columns[0]
                        class_distribution_data = clean_df[target_col].value_counts().to_dict()
                except:
                    pass
            
            if class_distribution_data:
                # Create pie chart
                fig, ax = plt.subplots(figsize=(8, 6))
                colors = ['#6bcf7f', '#ffd93d', '#ff6b6b']
                
                # Ensure we have the right order and colors
                classes = list(class_distribution_data.keys())
                counts = list(class_distribution_data.values())
                
                # Sort by count for consistent coloring
                sorted_data = sorted(zip(classes, counts), key=lambda x: x[1], reverse=True)
                classes_sorted = [x[0] for x in sorted_data]
                counts_sorted = [x[1] for x in sorted_data]
                
                wedges, texts, autotexts = ax.pie(
                    counts_sorted, 
                    labels=classes_sorted, 
                    autopct='%1.1f%%',
                    colors=colors[:len(classes_sorted)],
                    startangle=90
                )
                
                # Improve text appearance
                for autotext in autotexts:
                    autotext.set_color('white')
                    autotext.set_fontweight('bold')
                
                ax.set_title('Class Distribution', fontsize=14, fontweight='bold')
                plt.tight_layout()
                st.pyplot(fig)
                
                # Display statistics table
                st.markdown("**Class Statistics:**")
                stats_df = pd.DataFrame({
                    'Class': classes_sorted,
                    'Count': counts_sorted,
                    'Percentage': [f"{(count/sum(counts_sorted))*100:.1f}%" for count in counts_sorted]
                })
                st.dataframe(stats_df, use_container_width=True, hide_index=True)
                
            else:
                st.info("Class distribution data not available")
                st.markdown("""
                **Expected class distribution from training:**
                - **Graduate**: ~50% (2209 students)
                - **Dropout**: ~32% (1421 students) 
                - **Enrolled**: ~18% (794 students)
                """)
        
        with col4:
            st.markdown("#### Bootstrap Distribution")
            if 'bootstrap_results' in eval_res:
                bootstrap_scores = eval_res['bootstrap_results'].get('bootstrap_scores', [])
                if bootstrap_scores:
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.hist(bootstrap_scores, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
                    
                    mean_score = np.mean(bootstrap_scores)
                    ci_lower = np.percentile(bootstrap_scores, 2.5)
                    ci_upper = np.percentile(bootstrap_scores, 97.5)
                    
                    ax.axvline(mean_score, color='red', linestyle='--', linewidth=2, 
                              label=f"Mean: {mean_score:.4f}")
                    ax.axvline(ci_lower, color='orange', linestyle='--', linewidth=1,
                              label=f"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]")
                    ax.axvline(ci_upper, color='orange', linestyle='--', linewidth=1)
                    
                    ax.set_xlabel('F1 Score (Weighted)')
                    ax.set_ylabel('Frequency')
                    ax.set_title('Bootstrap Distribution of F1 Scores')
                    ax.legend()
                    plt.tight_layout()
                    st.pyplot(fig)
                else:
                    st.info("Bootstrap results not available")
            else:
                st.info("Bootstrap distribution visualization not available")

# -------------------------------------------------
# PAGE 4: Feature Insights
# -------------------------------------------------
elif page == "Feature Insights":
    st.markdown('<h1 class="main-header">Feature Insights</h1>', unsafe_allow_html=True)
    
    if model is None:
        st.error("Model not loaded. Please train the model first from the 'Retrain Model' page.")
    else:
        st.markdown("""
        ### How Features Determine Outcomes
        Based on the trained Random Forest model, here's how different features influence predictions:
        """)
        
        with st.expander("GRADUATE-Determining Features", expanded=True):
            st.markdown("""
            **Primary Success Indicators:**
            - **Curricular units 2nd sem (approved)** - High approval rate (≥5)
            - **Curricular units 2nd sem (grade)** - Strong grades (≥14/20)
            - **Curricular units 1st sem (approved)** - Good 1st semester performance
            - **Tuition fees up to date** - No financial barriers
            - **Scholarship holder** - Financial support available
            - **Admission grade** - High entrance scores
            **Students with these characteristics are likely to graduate successfully.**
            """)
        
        with st.expander("DROPOUT-Determining Features"):
            st.markdown("""
            **Critical Risk Indicators:**
            - **Curricular units 2nd sem (approved)** - Very low approvals (≤1)
            - **Curricular units 2nd sem (grade)** - Poor grades (≤8/20)
            - **Tuition fees up to date = 0** - Financial difficulties
            - **Debtor = 1** - Outstanding debts
            - **High Age at enrollment** (>25 years)
            - **Low previous qualification grades**
            **Students with these characteristics need immediate intervention.**
            """)
        
        with st.expander("ENROLLED-Determined Features"):
            st.markdown("""
            **Continuing Student Profile:**
            - **Curricular units 2nd sem (approved)** - Moderate (2-4 approvals)
            - **Curricular units 2nd sem (grade)** - Average (10-13/20)
            - **Mixed financial situation** - Fees paid but academic challenges
            - **Average family background** - Moderate parental education
            - **Typical age range** (18-22 years)
            **These students are progressing but need monitoring and support.**
            """)
        
        st.markdown("---")
        st.subheader("Quick Decision Guide")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("**DROPOUT if:**\n- 2nd sem approved ≤ 1\n- 2nd sem grade ≤ 8\n- Fees unpaid + debtor")
        with col2:
            st.markdown("**ENROLLED if:**\n- 2nd sem approved = 2-4\n- 2nd sem grade = 10-13\n- Fees paid but moderate performance")
        with col3:
            st.markdown("**GRADUATE if:**\n- 2nd sem approved ≥ 5\n- 2nd sem grade ≥ 14\n- Fees paid + no debt")
        
        st.markdown("---")
        st.subheader("Feature Importance Ranking")
        if hasattr(model, 'feature_importances_'):
            feature_importances = model.feature_importances_
            feat_imp_df = pd.DataFrame({"Feature": features, "Importance": feature_importances}).sort_values("Importance", ascending=True).tail(15)
            fig, ax = plt.subplots(figsize=(10, 8))
            bars = ax.barh(feat_imp_df['Feature'], feat_imp_df['Importance'], color='skyblue')
            ax.set_xlabel('Importance Score')
            ax.set_title('Top 15 Most Important Features')
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 0.001, bar.get_y() + bar.get_height()/2, f'{width:.3f}', ha='left', va='center')
            plt.tight_layout()
            st.pyplot(fig)
        else:
            st.info("Feature importance data not available")

# -------------------------------------------------
# PAGE 5: Training Outputs
# -------------------------------------------------
elif page == "Training Outputs":
    st.markdown('<h1 class="main-header">Training Process Outputs</h1>', unsafe_allow_html=True)
    
    # Training Summary
    st.markdown("### Training Summary")
    
    # Get dynamic metrics
    parsed_metrics = training_outputs.get('parsed_metrics', {})
    dataset_stats = training_outputs.get('dataset_stats', {})
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        original_shape = parsed_metrics.get('original_shape', (0, 0))
        final_shape = dataset_stats.get('final_shape', (0, 0))
        
        st.metric("Original Dataset", f"{original_shape[0]} students" if original_shape[0] > 0 else "N/A")
        st.metric("Final Dataset", f"{final_shape[0]} students" if final_shape[0] > 0 else "N/A")
        st.metric("Features Selected", f"{len(features)} out of {original_shape[1] if original_shape[1] > 0 else 'N/A'}")
    
    with col2:
        st.metric("Training Set", "70%")
        st.metric("Test Set", "30%")
        st.metric("Best Model", "Random Forest")
    
    with col3:
        cv_score = parsed_metrics.get('cv_mean', eval_res.get('cv_f1_macro_mean'))
        holdout_acc = parsed_metrics.get('holdout_accuracy', eval_res.get('holdout_accuracy'))
        bootstrap_f1 = eval_res.get('bootstrap_results', {}).get('mean_f1_weighted')
        
        st.metric("CV Score (F1-macro)", f"{cv_score:.3f}" if cv_score else "N/A")
        st.metric("Holdout Accuracy", f"{holdout_acc:.3f}" if holdout_acc else "N/A")
        
        if bootstrap_f1 and 'bootstrap_results' in eval_res:
            bootstrap_std = eval_res['bootstrap_results'].get('bootstrap_std', 0)
            st.metric("Bootstrap F1", f"{bootstrap_f1:.3f} ± {bootstrap_std:.3f}")
        else:
            st.metric("Bootstrap F1", "N/A")
    
    # Display actual training outputs from files
    st.markdown("---")
    st.markdown("### Actual Training Outputs")
    
    # Display missing values report if available
    if training_outputs['missing_values'] is not None and not training_outputs['missing_values'].empty:
        with st.expander("Missing Values Report", expanded=True):
            st.dataframe(training_outputs['missing_values'], use_container_width=True)
    
    # Display target distribution
    if dataset_stats.get('target_distribution'):
        with st.expander("Target Variable Distribution", expanded=True):
            target_dist = dataset_stats['target_distribution']
            st.write("**Class Distribution:**")
            for class_name, count in target_dist.items():
                st.write(f"- {class_name}: {count} ({count/sum(target_dist.values()):.1%})")
            
            # Plot distribution
            fig, ax = plt.subplots(figsize=(10, 6))
            colors = ['#6bcf7f', '#ffd93d', '#ff6b6b']
            ax.bar(target_dist.keys(), target_dist.values(), color=colors[:len(target_dist)])
            ax.set_ylabel('Count')
            ax.set_title('Target Variable Distribution')
            plt.xticks(rotation=45)
            plt.tight_layout()
            st.pyplot(fig)
    
    # Display selected features
    if os.path.exists('model/selected_features.csv'):
        with st.expander("Selected Features", expanded=True):
            selected_df = pd.read_csv('model/selected_features.csv')
            st.write(f"**Total Features Selected:** {len(selected_df)}")
            st.dataframe(selected_df, use_container_width=True)
    
    # Display evaluation results
    if eval_res:
        with st.expander("Model Evaluation Results", expanded=True):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Performance Metrics**")
                if 'holdout_accuracy' in eval_res:
                    st.metric("Holdout Accuracy", f"{eval_res['holdout_accuracy']:.3f}")
                if 'holdout_f1_weighted' in eval_res:
                    st.metric("F1 Score (Weighted)", f"{eval_res['holdout_f1_weighted']:.3f}")
                if 'cv_f1_macro_mean' in eval_res:
                    st.metric("CV F1-macro Mean", f"{eval_res['cv_f1_macro_mean']:.3f}")
                if 'cv_f1_macro_std' in eval_res:
                    st.metric("CV F1-macro Std", f"{eval_res['cv_f1_macro_std']:.3f}")
            
            with col2:
                st.markdown("**Best Hyperparameters**")
                if 'best_params' in eval_res:
                    for param, value in eval_res['best_params'].items():
                        st.write(f"- **{param}**: `{value}`")
                else:
                    st.write("No hyperparameters saved in evaluation results")
            
            # Display confusion matrix if available
            if 'confusion_matrix' in eval_res:
                st.markdown("**Confusion Matrix**")
                cm = np.array(eval_res['confusion_matrix'])
                
                if hasattr(encoder, 'classes_'):
                    classes = encoder.classes_
                else:
                    classes = [f"Class {i}" for i in range(len(cm))]
                
                fig, ax = plt.subplots(figsize=(8, 6))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                           xticklabels=classes, yticklabels=classes, ax=ax)
                ax.set_xlabel('Predicted')
                ax.set_ylabel('Actual')
                ax.set_title('Confusion Matrix')
                plt.tight_layout()
                st.pyplot(fig)

# -------------------------------------------------
# PAGE 6: Retrain Model
# -------------------------------------------------
elif page == "Retrain Model":
    st.markdown('<h1 class="main-header">Retrain Model</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    ### Model Retraining
    Click the button below to retrain the model with the latest data. This will:
    - Reload and preprocess the data
    - Perform feature selection
    - Train a new Random Forest model
    - Update all model artifacts
    - Generate new visualizations
    """)
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if st.button("Start Model Retraining", type="primary", use_container_width=True):
            training_output = run_training_script()
            
            if training_output:
                st.markdown("### Training Output")
                st.markdown('<div class="training-output">', unsafe_allow_html=True)
                st.text(training_output)
                st.markdown('</div>', unsafe_allow_html=True)
                
                # Reload artifacts after training
                st.runtime.legacy_caching.clear_cache()
                st.success("Model artifacts updated! Refresh the page to see changes.")
    
    with col2:
        st.markdown("### Current Model Info")
        st.write(f"**Features:** {len(features)}")
        if hasattr(encoder, 'classes_'):
            st.write(f"**Classes:** {list(encoder.classes_)}")
        
        if os.path.exists('data.csv'):
            df_info = pd.read_csv('data.csv')
            st.write(f"**Dataset:** {df_info.shape[0]} rows, {df_info.shape[1]} columns")
        
        st.markdown("---")
        st.markdown("**Last Training:**")
        if os.path.exists('model/rf_model.pkl'):
            model_time = os.path.getmtime('model/rf_model.pkl')
            st.write(f"{datetime.fromtimestamp(model_time).strftime('%Y-%m-%d %H:%M')}")

# -------------------------------------------------
# Footer
# -------------------------------------------------
st.markdown("---")
st.markdown(f"""
<div style='text-align: center; color: gray;'>
    <p><b>Student Dropout Prediction System</b> | BIAI 3110 Project | Limkokwing University</p>
    <p>Powered by Random Forest Machine Learning | {datetime.now().strftime('%Y')}</p>
</div>
""", unsafe_allow_html=True)